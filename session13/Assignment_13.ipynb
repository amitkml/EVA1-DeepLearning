{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 13.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmLLIibCa3Df",
        "colab_type": "text"
      },
      "source": [
        "Weight decay is set to 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0_OSDaU3Vha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "642b896c-7758-44c9-fc3a-97516cce7308"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib as tf_contrib\n",
        "\n",
        "\n",
        "# Xavier : tf_contrib.layers.xavier_initializer()\n",
        "# He : tf_contrib.layers.variance_scaling_initializer()\n",
        "# Normal : tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "# l2_decay : tf_contrib.layers.l2_regularizer(0.0001)\n",
        "\n",
        "weight_init = tf_contrib.layers.variance_scaling_initializer()\n",
        "weight_regularizer = tf_contrib.layers.l2_regularizer(5e-4)\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "# Layer\n",
        "##################################################################################\n",
        "\n",
        "def conv(x, channels, kernel=4, stride=2, padding='SAME', use_bias=True, scope='conv_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "        x = tf.layers.conv2d(inputs=x, filters=channels,\n",
        "                             kernel_size=kernel, kernel_initializer=weight_init,\n",
        "                             kernel_regularizer=weight_regularizer,\n",
        "                             strides=stride, use_bias=use_bias, padding=padding)\n",
        "\n",
        "        return x\n",
        "\n",
        "def fully_conneted(x, units, use_bias=True, scope='fully_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "        x = flatten(x)\n",
        "        x = tf.layers.dense(x, units=units, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, use_bias=use_bias)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resblock(x_init, channels, is_training=True, use_bias=True, downsample=False, scope='resblock') :\n",
        "    with tf.variable_scope(scope) :\n",
        "\n",
        "        x = batch_norm(x_init, is_training, scope='batch_norm_0')\n",
        "        x = relu(x)\n",
        "\n",
        "\n",
        "        if downsample :\n",
        "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
        "            x_init = conv(x_init, channels, kernel=1, stride=2, use_bias=use_bias, scope='conv_init')\n",
        "\n",
        "        else :\n",
        "            x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_0')\n",
        "\n",
        "        x = batch_norm(x, is_training, scope='batch_norm_1')\n",
        "        x = relu(x)\n",
        "        x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_1')\n",
        "\n",
        "\n",
        "\n",
        "        return x + x_init\n",
        "\n",
        "def bottle_resblock(x_init, channels, is_training=True, use_bias=True, downsample=False, scope='bottle_resblock') :\n",
        "    with tf.variable_scope(scope) :\n",
        "        x = batch_norm(x_init, is_training, scope='batch_norm_1x1_front')\n",
        "        shortcut = relu(x)\n",
        "\n",
        "        x = conv(shortcut, channels, kernel=1, stride=1, use_bias=use_bias, scope='conv_1x1_front')\n",
        "        x = batch_norm(x, is_training, scope='batch_norm_3x3')\n",
        "        x = relu(x)\n",
        "\n",
        "        if downsample :\n",
        "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
        "            shortcut = conv(shortcut, channels*4, kernel=1, stride=2, use_bias=use_bias, scope='conv_init')\n",
        "\n",
        "        else :\n",
        "            x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_0')\n",
        "            shortcut = conv(shortcut, channels * 4, kernel=1, stride=1, use_bias=use_bias, scope='conv_init')\n",
        "\n",
        "        x = batch_norm(x, is_training, scope='batch_norm_1x1_back')\n",
        "        x = relu(x)\n",
        "        x = conv(x, channels*4, kernel=1, stride=1, use_bias=use_bias, scope='conv_1x1_back')\n",
        "\n",
        "        return x + shortcut\n",
        "\n",
        "\n",
        "\n",
        "def get_residual_layer(res_n) :\n",
        "    x = []\n",
        "\n",
        "    if res_n == 18 :\n",
        "        x = [2, 2, 2, 2]\n",
        "\n",
        "    if res_n == 34 :\n",
        "        x = [3, 4, 6, 3]\n",
        "\n",
        "    if res_n == 50 :\n",
        "        x = [3, 4, 6, 3]\n",
        "\n",
        "    if res_n == 101 :\n",
        "        x = [3, 4, 23, 3]\n",
        "\n",
        "    if res_n == 152 :\n",
        "        x = [3, 8, 36, 3]\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "# Sampling\n",
        "##################################################################################\n",
        "\n",
        "def flatten(x) :\n",
        "    return tf.layers.flatten(x)\n",
        "\n",
        "def global_avg_pooling(x):\n",
        "    gap = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
        "    return gap\n",
        "\n",
        "def avg_pooling(x) :\n",
        "    return tf.layers.average_pooling2d(x, pool_size=2, strides=2, padding='SAME')\n",
        "\n",
        "##################################################################################\n",
        "# Activation function\n",
        "##################################################################################\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "# Normalization function\n",
        "##################################################################################\n",
        "\n",
        "def batch_norm(x, is_training=True, scope='batch_norm'):\n",
        "    return tf_contrib.layers.batch_norm(x,\n",
        "                                        decay=0.9, epsilon=1e-05,\n",
        "                                        center=True, scale=True, updates_collections=None,\n",
        "                                        is_training=is_training, scope=scope)\n",
        "\n",
        "##################################################################################\n",
        "# Loss function\n",
        "##################################################################################\n",
        "\n",
        "def classification_loss(logit, label) :\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logit))\n",
        "    prediction = tf.equal(tf.argmax(logit, -1), tf.argmax(label, -1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
        "\n",
        "    return loss, accuracy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oz53ztmbA2N",
        "colab_type": "text"
      },
      "source": [
        "Use Normalization values of: (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "Random Crop of 32 with padding of 4px\n",
        "\n",
        "Horizontal Flip (0.5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q13ndka3av_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75013dcd-20cb-43b6-b12d-d60c65bfb055"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import os\n",
        "from keras.datasets import cifar10, cifar100, mnist, fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy import misc\n",
        "\n",
        "def check_folder(log_dir):\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    return log_dir\n",
        "\n",
        "def show_all_variables():\n",
        "    model_vars = tf.trainable_variables()\n",
        "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
        "\n",
        "def str2bool(x):\n",
        "    return x.lower() in ('true')\n",
        "\n",
        "def load_cifar10() :\n",
        "    (train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n",
        "    # train_data = train_data / 255.0\n",
        "    # test_data = test_data / 255.0\n",
        "\n",
        "    train_data, test_data = normalize(train_data, test_data)\n",
        "\n",
        "    train_labels = to_categorical(train_labels, 10)\n",
        "    test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "    seed = 777\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(train_data)\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(train_labels)\n",
        "\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "\n",
        "\n",
        "def normalize(X_train, X_test):\n",
        "\n",
        "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
        "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "    print(mean)\n",
        "    print(std)\n",
        "    MEAN_IMAGE = tf.constant([0.4914, 0.4822, 0.4465], dtype=tf.float32)\n",
        "    STD_IMAGE = tf.constant([0.2023, 0.1994, 0.2010], dtype=tf.float32)\n",
        "    # showing shape not \n",
        "    \n",
        "    X_train = (X_train/255 - [0.4914, 0.4822, 0.4465]) / [0.2023, 0.1994, 0.2010]\n",
        "    X_test = (X_test/255 - [0.4914, 0.4822, 0.4465]) / [0.2023, 0.1994, 0.2010]\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "def get_annotations_map():\n",
        "    valAnnotationsPath = './tiny-imagenet-200/val/val_annotations.txt'\n",
        "    valAnnotationsFile = open(valAnnotationsPath, 'r')\n",
        "    valAnnotationsContents = valAnnotationsFile.read()\n",
        "    valAnnotations = {}\n",
        "\n",
        "    for line in valAnnotationsContents.splitlines():\n",
        "        pieces = line.strip().split()\n",
        "        valAnnotations[pieces[0]] = pieces[1]\n",
        "\n",
        "    return valAnnotations\n",
        "\n",
        "def _random_crop(batch, crop_shape, padding=None):\n",
        "    oshape = np.shape(batch[0])\n",
        "\n",
        "    if padding:\n",
        "        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)\n",
        "    new_batch = []\n",
        "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
        "    for i in range(len(batch)):\n",
        "        new_batch.append(batch[i])\n",
        "        if padding:\n",
        "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
        "                                      mode='constant', constant_values=0)\n",
        "        nh = random.randint(0, oshape[0] - crop_shape[0])\n",
        "        nw = random.randint(0, oshape[1] - crop_shape[1])\n",
        "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
        "                       nw:nw + crop_shape[1]]\n",
        "    return new_batch\n",
        "\n",
        "\n",
        "def _random_flip_leftright(batch):\n",
        "    for i in range(len(batch)):\n",
        "        if bool(random.getrandbits(1)):\n",
        "            batch[i] = np.fliplr(batch[i])\n",
        "    return batch\n",
        "\n",
        "def data_augmentation(batch, img_size, dataset_name):\n",
        "    if dataset_name == 'mnist' :\n",
        "        batch = _random_crop(batch, [img_size, img_size], 4)\n",
        "\n",
        "    elif dataset_name =='tiny' :\n",
        "        batch = _random_flip_leftright(batch)\n",
        "        batch = _random_crop(batch, [img_size, img_size], 8)\n",
        "\n",
        "    else :\n",
        "        batch = _random_flip_leftright(batch)\n",
        "        batch = _random_crop(batch, [img_size, img_size], 4)\n",
        "    return batch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWIjVcKObU2N",
        "colab_type": "text"
      },
      "source": [
        "Optimizer: SGD\n",
        "\n",
        " Use Batch Size 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F_Q8RJ8d7Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyWkudxgd8YV",
        "colab_type": "text"
      },
      "source": [
        "B1 -  64 channels\n",
        "\n",
        "B2  - 128 channels\n",
        "\n",
        "B3  - 256 channels\n",
        "\n",
        "B4  - 512 channels\n",
        "\n",
        "Are used with stride of 1 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6dEwtsfzES-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "# from ops import *\n",
        "# from utils import *\n",
        "\n",
        "class ResNet(object):\n",
        "    def __init__(self, sess, args):\n",
        "        self.model_name = 'ResNet'\n",
        "        self.sess = sess\n",
        "        # self.dataset_name = args.dataset\n",
        "        self.dataset_name = 'cifar10'\n",
        "\n",
        "        if self.dataset_name == 'cifar10' :\n",
        "            self.train_x, self.train_y, self.test_x, self.test_y = load_cifar10()\n",
        "            self.img_size = 32\n",
        "            self.c_dim = 3\n",
        "            self.label_dim = 10\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "        self.checkpoint_dir ='/' #args.checkpoint_dir\n",
        "        self.log_dir = '/' #args.log_dir\n",
        "\n",
        "        self.res_n =18 # args.res_n\n",
        "\n",
        "        self.epoch =80 #args.epoch\n",
        "        self.batch_size = 128 #args.batch_size\n",
        "        self.iteration = len(self.train_x) // self.batch_size\n",
        "\n",
        "        self.init_lr = 0.01 #args.lr\n",
        "\n",
        "\n",
        "    ##################################################################################\n",
        "    # Generator\n",
        "    ##################################################################################\n",
        "    \n",
        "    \n",
        "\n",
        "    def network(self, x, is_training=True, reuse=False):\n",
        "        \n",
        "        with tf.variable_scope(\"network\", reuse=reuse):\n",
        "\n",
        "            if self.res_n < 50 :\n",
        "                residual_block = resblock\n",
        "            else :\n",
        "                residual_block = bottle_resblock\n",
        "\n",
        "            residual_list = get_residual_layer(self.res_n)\n",
        "\n",
        "            ch = 64 # paper is 64\n",
        "\n",
        "            \n",
        "\n",
        "            x = conv(x, channels=ch, kernel=3, stride=1, scope='conv')\n",
        "\n",
        "            for i in range(residual_list[0]) :\n",
        "                x = residual_block(x, channels=ch, is_training=is_training, downsample=False, scope='resblock0_' + str(i))\n",
        "\n",
        "            ########################################################################################################\n",
        "\n",
        "            x = residual_block(x, channels=ch*2, is_training=is_training, downsample=True, scope='resblock1_0')\n",
        "\n",
        "            for i in range(1, residual_list[1]) :\n",
        "                x = residual_block(x, channels=ch*2, is_training=is_training, downsample=False, scope='resblock1_' + str(i))\n",
        "\n",
        "            ########################################################################################################\n",
        "\n",
        "            x = residual_block(x, channels=ch*4, is_training=is_training, downsample=True, scope='resblock2_0')\n",
        "\n",
        "            for i in range(1, residual_list[2]) :\n",
        "                x = residual_block(x, channels=ch*4, is_training=is_training, downsample=False, scope='resblock2_' + str(i))\n",
        "\n",
        "            ########################################################################################################\n",
        "\n",
        "            x = residual_block(x, channels=ch*8, is_training=is_training, downsample=True, scope='resblock_3_0')\n",
        "\n",
        "            for i in range(1, residual_list[3]) :\n",
        "                x = residual_block(x, channels=ch*8, is_training=is_training, downsample=False, scope='resblock_3_' + str(i))\n",
        "\n",
        "            ########################################################################################################\n",
        "\n",
        "\n",
        "            x = batch_norm(x, is_training, scope='batch_norm')\n",
        "            x = relu(x)\n",
        "\n",
        "            x = global_avg_pooling(x)\n",
        "            x = fully_conneted(x, units=self.label_dim, scope='logit')\n",
        "\n",
        "            return x\n",
        "\n",
        "    ##################################################################################\n",
        "    # Model\n",
        "    ##################################################################################\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\" Graph Input \"\"\"\n",
        "        self.train_inptus = tf.placeholder(tf.float32, [self.batch_size, self.img_size, self.img_size, self.c_dim], name='train_inputs')\n",
        "        self.train_labels = tf.placeholder(tf.float32, [self.batch_size, self.label_dim], name='train_labels')\n",
        "\n",
        "        self.test_inptus = tf.placeholder(tf.float32, [len(self.test_x), self.img_size, self.img_size, self.c_dim], name='test_inputs')\n",
        "        self.test_labels = tf.placeholder(tf.float32, [len(self.test_y), self.label_dim], name='test_labels')\n",
        "\n",
        "        self.lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "\n",
        "        \"\"\" Model \"\"\"\n",
        "        self.train_logits = self.network(self.train_inptus)\n",
        "        self.test_logits = self.network(self.test_inptus, is_training=False, reuse=True)\n",
        "\n",
        "        self.train_loss, self.train_accuracy = classification_loss(logit=self.train_logits, label=self.train_labels)\n",
        "        self.test_loss, self.test_accuracy = classification_loss(logit=self.test_logits, label=self.test_labels)\n",
        "        \n",
        "        reg_loss = tf.losses.get_regularization_loss()\n",
        "        self.train_loss += reg_loss\n",
        "        self.test_loss += reg_loss\n",
        "\n",
        "\n",
        "        \"\"\" Training \"\"\"\n",
        "        self.optim = tf.train.MomentumOptimizer(self.lr, momentum=0.9).minimize(self.train_loss)\n",
        "\n",
        "        \"\"\"\" Summary \"\"\"\n",
        "        self.summary_train_loss = tf.summary.scalar(\"train_loss\", self.train_loss)\n",
        "        self.summary_train_accuracy = tf.summary.scalar(\"train_accuracy\", self.train_accuracy)\n",
        "\n",
        "        self.summary_test_loss = tf.summary.scalar(\"test_loss\", self.test_loss)\n",
        "        self.summary_test_accuracy = tf.summary.scalar(\"test_accuracy\", self.test_accuracy)\n",
        "\n",
        "        self.train_summary = tf.summary.merge([self.summary_train_loss, self.summary_train_accuracy])\n",
        "        self.test_summary = tf.summary.merge([self.summary_test_loss, self.summary_test_accuracy])\n",
        "\n",
        "    ##################################################################################\n",
        "    # Train\n",
        "    ##################################################################################\n",
        "\n",
        "    def train(self):\n",
        "        # initialize all variables\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        # saver to save model\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # summary writer\n",
        "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\n",
        "\n",
        "        # restore check-point if it exits\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "        if could_load:\n",
        "            epoch_lr = self.init_lr\n",
        "            start_epoch = (int)(checkpoint_counter / self.iteration)\n",
        "            start_batch_id = checkpoint_counter - start_epoch * self.iteration\n",
        "            counter = checkpoint_counter\n",
        "\n",
        "            if start_epoch >= int(self.epoch * 0.75) :\n",
        "                epoch_lr = epoch_lr * 0.01\n",
        "            elif start_epoch >= int(self.epoch * 0.5) and start_epoch < int(self.epoch * 0.75) :\n",
        "                epoch_lr = epoch_lr * 0.1\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            epoch_lr = self.init_lr\n",
        "            start_epoch = 0\n",
        "            start_batch_id = 0\n",
        "            counter = 1\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        # loop for epoch\n",
        "        start_time = time.time()\n",
        "        for epoch in range(start_epoch, self.epoch):\n",
        "            if epoch == int(self.epoch * 0.5) or epoch == int(self.epoch * 0.75) :\n",
        "                epoch_lr = epoch_lr * 0.1\n",
        "            print(\"Epoch: [%2d]  time: %4.4f, learning_rate : %.4f\" \\\n",
        "                      % (epoch,   time.time() - start_time, epoch_lr))\n",
        "            # get batch data\n",
        "            for idx in range(start_batch_id, self.iteration):\n",
        "                batch_x = self.train_x[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "                batch_y = self.train_y[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "                batch_x = data_augmentation(batch_x, self.img_size, self.dataset_name)\n",
        "\n",
        "                train_feed_dict = {\n",
        "                    self.train_inptus : batch_x,\n",
        "                    self.train_labels : batch_y,\n",
        "                    self.lr : epoch_lr\n",
        "                }\n",
        "\n",
        "                test_feed_dict = {\n",
        "                    self.test_inptus : self.test_x,\n",
        "                    self.test_labels : self.test_y\n",
        "                }\n",
        "\n",
        "\n",
        "                # update network\n",
        "                _, summary_str, train_loss, train_accuracy = self.sess.run(\n",
        "                    [self.optim, self.train_summary, self.train_loss, self.train_accuracy], feed_dict=train_feed_dict)\n",
        "                self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                # test\n",
        "                summary_str, test_loss, test_accuracy = self.sess.run(\n",
        "                    [self.test_summary, self.test_loss, self.test_accuracy], feed_dict=test_feed_dict)\n",
        "                self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                # display training status\n",
        "                counter += 1\n",
        "            print(\"Epoch: [%2d] [%5d/%5d] time: %4.4f, train_accuracy: %.2f, test_accuracy: %.2f, learning_rate : %.4f\" \\\n",
        "                      % (epoch, idx, self.iteration, time.time() - start_time, train_accuracy, test_accuracy, epoch_lr))\n",
        "\n",
        "            # After an epoch, start_batch_id is set to zero\n",
        "            # non-zero value is only for the first epoch after loading pre-trained model\n",
        "            start_batch_id = 0\n",
        "\n",
        "            # save model\n",
        "            self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "        # save model for final step\n",
        "        self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "    @property\n",
        "    def model_dir(self):\n",
        "        return \"{}{}_{}_{}_{}\".format(self.model_name, self.res_n, self.dataset_name, self.batch_size, self.init_lr)\n",
        "\n",
        "    def save(self, checkpoint_dir, step):\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name+'.model'), global_step=step)\n",
        "\n",
        "    def load(self, checkpoint_dir):\n",
        "        print(\" [*] Reading checkpoints...\")\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
        "\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
        "            counter = int(ckpt_name.split('-')[-1])\n",
        "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
        "            return True, counter\n",
        "        else:\n",
        "            print(\" [*] Failed to find a checkpoint\")\n",
        "            return False, 0\n",
        "\n",
        "    def test(self):\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        self.saver = tf.train.Saver()\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "\n",
        "        if could_load:\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        test_feed_dict = {\n",
        "            self.test_inptus: self.test_x,\n",
        "            self.test_labels: self.test_y\n",
        "        }\n",
        "\n",
        "\n",
        "        test_accuracy = self.sess.run(self.test_accuracy, feed_dict=test_feed_dict)\n",
        "        print(\"test_accuracy: {}\".format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6F_5rB85Jcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f506e11-b7f9-44b1-e09a-f9ea8aae57f9"
      },
      "source": [
        "!pip install utils"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (0.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZtgXAh68iWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a638c230-c243-4902-c4c0-884ab306bd83"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw_FjUdPUeKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b988440-de3e-48ef-c084-a9a7a767e94b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhaHRdX33hik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a17aa7a7-85b7-4dac-b099-45ffa816749f"
      },
      "source": [
        "#from ResNet import ResNet\n",
        "import argparse\n",
        "from utils import *\n",
        "\n",
        "\"\"\"parsing and configuration\"\"\"\n",
        "def parse_args():\n",
        "    desc = \"Tensorflow implementation of ResNet\"\n",
        "    parser = argparse.ArgumentParser(description=desc)\n",
        "    parser.add_argument('--phase', type=str, default='train', help='train or test ?')\n",
        "    parser.add_argument('--dataset', type=str, default='tiny', help='[cifar10, cifar100, mnist, fashion-mnist, tiny')\n",
        "\n",
        "\n",
        "    parser.add_argument('--epoch', type=int, default=82, help='The number of epochs to run')\n",
        "    parser.add_argument('--batch_size', type=int, default=256, help='The size of batch per gpu')\n",
        "    parser.add_argument('--res_n', type=int, default=18, help='18, 34, 50, 101, 152')\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=0.1, help='learning rate')\n",
        "\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',\n",
        "                        help='Directory name to save the checkpoints')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs',\n",
        "                        help='Directory name to save training logs')\n",
        "\n",
        "    return check_args(parser.parse_args())\n",
        "\n",
        "\"\"\"checking arguments\"\"\"\n",
        "def check_args(args):\n",
        "    # --checkpoint_dir\n",
        "    check_folder(args.checkpoint_dir)\n",
        "\n",
        "    # --result_dir\n",
        "    check_folder(args.log_dir)\n",
        "\n",
        "    # --epoch\n",
        "    try:\n",
        "        assert args.epoch >= 1\n",
        "    except:\n",
        "        print('number of epochs must be larger than or equal to one')\n",
        "\n",
        "    # --batch_size\n",
        "    try:\n",
        "        assert args.batch_size >= 1\n",
        "    except:\n",
        "        print('batch size must be larger than or equal to one')\n",
        "    return args\n",
        "\n",
        "\n",
        "\"\"\"main\"\"\"\n",
        "def main():\n",
        "    # parse arguments\n",
        "    #args = parse_args()\n",
        "    # if args is None:\n",
        "    #   exit()\n",
        "\n",
        "    # open session\n",
        "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        args=()\n",
        "        cnn = ResNet(sess, args)\n",
        "\n",
        "        # build graph\n",
        "        cnn.build_model()\n",
        "\n",
        "        # show network architecture\n",
        "        show_all_variables()\n",
        "\n",
        "        #if args.phase == 'train' :\n",
        "        # launch the graph in a session\n",
        "        cnn.train()\n",
        "\n",
        "        print(\" [*] Training finished! \\n\")\n",
        "\n",
        "        cnn.test()\n",
        "        print(\" [*] Test finished!\")\n",
        "\n",
        "        # if args.phase == 'test' :\n",
        "        #     cnn.test()\n",
        "        #     print(\" [*] Test finished!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120.70756512369792\n",
            "64.1500758911213\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "WARNING:tensorflow:From <ipython-input-2-c155637b2286>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-c155637b2286>:108: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-c155637b2286>:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "network/conv/conv2d/kernel:0 (float32_ref 3x3x3x64) [1728, bytes: 6912]\n",
            "network/conv/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_0/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_0/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_0/conv_0/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_0/conv_1/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_0/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_0/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_1/conv_0/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_1/conv_1/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock1_0/batch_norm_0/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock1_0/batch_norm_0/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock1_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
            "network/resblock1_0/conv_0/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/conv_init/conv2d/kernel:0 (float32_ref 1x1x64x128) [8192, bytes: 32768]\n",
            "network/resblock1_0/conv_init/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/batch_norm_1/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/batch_norm_1/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
            "network/resblock1_0/conv_1/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_0/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_0/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
            "network/resblock1_1/conv_0/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_1/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_1/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
            "network/resblock1_1/conv_1/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock2_0/batch_norm_0/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock2_0/batch_norm_0/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock2_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x128x256) [294912, bytes: 1179648]\n",
            "network/resblock2_0/conv_0/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/conv_init/conv2d/kernel:0 (float32_ref 1x1x128x256) [32768, bytes: 131072]\n",
            "network/resblock2_0/conv_init/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/batch_norm_1/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/batch_norm_1/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
            "network/resblock2_0/conv_1/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_0/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_0/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
            "network/resblock2_1/conv_0/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_1/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_1/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
            "network/resblock2_1/conv_1/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock_3_0/batch_norm_0/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock_3_0/batch_norm_0/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock_3_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x256x512) [1179648, bytes: 4718592]\n",
            "network/resblock_3_0/conv_0/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/conv_init/conv2d/kernel:0 (float32_ref 1x1x256x512) [131072, bytes: 524288]\n",
            "network/resblock_3_0/conv_init/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/batch_norm_1/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/batch_norm_1/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
            "network/resblock_3_0/conv_1/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_0/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_0/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
            "network/resblock_3_1/conv_0/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_1/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_1/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
            "network/resblock_3_1/conv_1/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/batch_norm/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/batch_norm/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/logit/dense/kernel:0 (float32_ref 512x10) [5120, bytes: 20480]\n",
            "network/logit/dense/bias:0 (float32_ref 10) [10, bytes: 40]\n",
            "Total size of variables: 11176970\n",
            "Total bytes of variables: 44707880\n",
            " [*] Reading checkpoints...\n",
            "INFO:tensorflow:Restoring parameters from /ResNet18_cifar10_128_0.01/ResNet.model-3901\n",
            " [*] Success to read ResNet.model-3901\n",
            " [*] Load SUCCESS\n",
            "Epoch: [10]  time: 0.0000, learning_rate : 0.0100\n",
            "Epoch: [10] [  389/  390] time: 539.7299, train_accuracy: 0.73, test_accuracy: 0.72, learning_rate : 0.0100\n",
            "Epoch: [11]  time: 540.0521, learning_rate : 0.0100\n",
            "Epoch: [11] [  389/  390] time: 1076.2879, train_accuracy: 0.76, test_accuracy: 0.73, learning_rate : 0.0100\n",
            "Epoch: [12]  time: 1076.5210, learning_rate : 0.0100\n",
            "Epoch: [12] [  389/  390] time: 1612.9113, train_accuracy: 0.73, test_accuracy: 0.71, learning_rate : 0.0100\n",
            "Epoch: [13]  time: 1613.1384, learning_rate : 0.0100\n",
            "Epoch: [13] [  389/  390] time: 2149.1522, train_accuracy: 0.79, test_accuracy: 0.75, learning_rate : 0.0100\n",
            "Epoch: [14]  time: 2149.3709, learning_rate : 0.0100\n",
            "Epoch: [14] [  389/  390] time: 2685.1453, train_accuracy: 0.77, test_accuracy: 0.79, learning_rate : 0.0100\n",
            "Epoch: [15]  time: 2685.3544, learning_rate : 0.0100\n",
            "Epoch: [15] [  389/  390] time: 3221.1300, train_accuracy: 0.84, test_accuracy: 0.79, learning_rate : 0.0100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Epoch: [16]  time: 3221.3629, learning_rate : 0.0100\n",
            "Epoch: [16] [  389/  390] time: 3757.0530, train_accuracy: 0.83, test_accuracy: 0.80, learning_rate : 0.0100\n",
            "Epoch: [17]  time: 3757.2888, learning_rate : 0.0100\n",
            "Epoch: [17] [  389/  390] time: 4292.8599, train_accuracy: 0.84, test_accuracy: 0.81, learning_rate : 0.0100\n",
            "Epoch: [18]  time: 4293.0887, learning_rate : 0.0100\n",
            "Epoch: [18] [  389/  390] time: 4828.6429, train_accuracy: 0.82, test_accuracy: 0.82, learning_rate : 0.0100\n",
            "Epoch: [19]  time: 4828.8661, learning_rate : 0.0100\n",
            "Epoch: [19] [  389/  390] time: 5364.4659, train_accuracy: 0.80, test_accuracy: 0.83, learning_rate : 0.0100\n",
            "Epoch: [20]  time: 5364.6901, learning_rate : 0.0010\n",
            "Epoch: [20] [  389/  390] time: 5900.2703, train_accuracy: 0.88, test_accuracy: 0.87, learning_rate : 0.0010\n",
            "Epoch: [21]  time: 5900.4979, learning_rate : 0.0010\n",
            "Epoch: [21] [  389/  390] time: 6435.9824, train_accuracy: 0.86, test_accuracy: 0.88, learning_rate : 0.0010\n",
            "Epoch: [22]  time: 6436.2104, learning_rate : 0.0010\n",
            "Epoch: [22] [  389/  390] time: 6971.8350, train_accuracy: 0.88, test_accuracy: 0.88, learning_rate : 0.0010\n",
            "Epoch: [23]  time: 6972.0603, learning_rate : 0.0010\n",
            "Epoch: [23] [  389/  390] time: 7507.4535, train_accuracy: 0.88, test_accuracy: 0.88, learning_rate : 0.0010\n",
            "Epoch: [24]  time: 7507.6781, learning_rate : 0.0010\n",
            "Epoch: [24] [  389/  390] time: 8043.1462, train_accuracy: 0.93, test_accuracy: 0.88, learning_rate : 0.0010\n",
            "Epoch: [25]  time: 8043.3840, learning_rate : 0.0010\n",
            "Epoch: [25] [  389/  390] time: 8578.8412, train_accuracy: 0.89, test_accuracy: 0.88, learning_rate : 0.0010\n",
            "Epoch: [26]  time: 8579.0666, learning_rate : 0.0010\n",
            "Epoch: [26] [  389/  390] time: 9114.7606, train_accuracy: 0.89, test_accuracy: 0.88, learning_rate : 0.0010\n",
            "Epoch: [27]  time: 9115.0028, learning_rate : 0.0010\n",
            "Epoch: [27] [  389/  390] time: 9650.6848, train_accuracy: 0.91, test_accuracy: 0.88, learning_rate : 0.0010\n",
            "Epoch: [28]  time: 9650.9099, learning_rate : 0.0010\n",
            "Epoch: [28] [  389/  390] time: 10186.5291, train_accuracy: 0.89, test_accuracy: 0.88, learning_rate : 0.0010\n",
            "Epoch: [29]  time: 10186.7532, learning_rate : 0.0010\n",
            "Epoch: [29] [  389/  390] time: 10722.3555, train_accuracy: 0.91, test_accuracy: 0.88, learning_rate : 0.0010\n",
            "Epoch: [30]  time: 10722.5845, learning_rate : 0.0001\n",
            "Epoch: [30] [  389/  390] time: 11258.1158, train_accuracy: 0.93, test_accuracy: 0.88, learning_rate : 0.0001\n",
            "Epoch: [31]  time: 11258.3512, learning_rate : 0.0001\n",
            "Epoch: [31] [  389/  390] time: 11793.7855, train_accuracy: 0.91, test_accuracy: 0.88, learning_rate : 0.0001\n",
            "Epoch: [32]  time: 11794.0294, learning_rate : 0.0001\n",
            "Epoch: [32] [  389/  390] time: 12329.4873, train_accuracy: 0.91, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [33]  time: 12329.7190, learning_rate : 0.0001\n",
            "Epoch: [33] [  389/  390] time: 12865.1466, train_accuracy: 0.91, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [34]  time: 12865.3781, learning_rate : 0.0001\n",
            "Epoch: [34] [  389/  390] time: 13400.8784, train_accuracy: 0.92, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [35]  time: 13401.1072, learning_rate : 0.0001\n",
            "Epoch: [35] [  389/  390] time: 13936.6141, train_accuracy: 0.92, test_accuracy: 0.88, learning_rate : 0.0001\n",
            "Epoch: [36]  time: 13936.8419, learning_rate : 0.0001\n",
            "Epoch: [36] [  389/  390] time: 14472.3926, train_accuracy: 0.91, test_accuracy: 0.88, learning_rate : 0.0001\n",
            "Epoch: [37]  time: 14472.6256, learning_rate : 0.0001\n",
            "Epoch: [37] [  389/  390] time: 15008.1320, train_accuracy: 0.94, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [38]  time: 15008.3770, learning_rate : 0.0001\n",
            "Epoch: [38] [  389/  390] time: 15543.6401, train_accuracy: 0.89, test_accuracy: 0.88, learning_rate : 0.0001\n",
            "Epoch: [39]  time: 15543.8753, learning_rate : 0.0001\n",
            "Epoch: [39] [  389/  390] time: 16079.4588, train_accuracy: 0.93, test_accuracy: 0.88, learning_rate : 0.0001\n",
            " [*] Training finished! \n",
            "\n",
            " [*] Reading checkpoints...\n",
            "INFO:tensorflow:Restoring parameters from /ResNet18_cifar10_128_0.01/ResNet.model-15600\n",
            " [*] Success to read ResNet.model-15600\n",
            " [*] Load SUCCESS\n",
            "test_accuracy: 0.8849999904632568\n",
            " [*] Test finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e50cOy5USQF9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "16791172-25f4-4899-b855-7524c75a356b"
      },
      "source": [
        "!zip -r ResNet18_cifar10_128_0.01.zip ResNet18_cifar10_128_0.01"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: ResNet18_cifar10_128_0.01/ (stored 0%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3511.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2341.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3121.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2731.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2731.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/events.out.tfevents.1576989402.12f1060b182d (deflated 87%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3121.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3901.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2341.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3511.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2731.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3121.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3901.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3511.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/checkpoint (deflated 78%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3901.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2341.data-00000-of-00001 (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTDeRhgLRTAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6cb4f25-c29c-47ea-af15-086759c9a56b"
      },
      "source": [
        " with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        args=()\n",
        "        cnn = ResNet(sess, args)\n",
        "\n",
        "        # build graph\n",
        "        cnn.build_model()\n",
        "\n",
        "        # show network architecture\n",
        "        show_all_variables()\n",
        "\n",
        "        #if args.phase == 'train' :\n",
        "        # launch the graph in a session\n",
        "        cnn.train()\n",
        "\n",
        "        print(\" [*] Training finished! \\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120.70756512369792\n",
            "64.1500758911213\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "WARNING:tensorflow:From <ipython-input-1-c155637b2286>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-c155637b2286>:108: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-c155637b2286>:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "network/conv/conv2d/kernel:0 (float32_ref 3x3x3x64) [1728, bytes: 6912]\n",
            "network/conv/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_0/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_0/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_0/conv_0/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/batch_norm_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_0/conv_1/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_0/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_0/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_1/conv_0/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/batch_norm_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock0_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
            "network/resblock0_1/conv_1/conv2d/bias:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock1_0/batch_norm_0/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock1_0/batch_norm_0/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "network/resblock1_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
            "network/resblock1_0/conv_0/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/conv_init/conv2d/kernel:0 (float32_ref 1x1x64x128) [8192, bytes: 32768]\n",
            "network/resblock1_0/conv_init/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/batch_norm_1/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/batch_norm_1/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
            "network/resblock1_0/conv_1/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_0/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_0/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
            "network/resblock1_1/conv_0/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_1/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/batch_norm_1/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock1_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x128x128) [147456, bytes: 589824]\n",
            "network/resblock1_1/conv_1/conv2d/bias:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock2_0/batch_norm_0/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock2_0/batch_norm_0/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "network/resblock2_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x128x256) [294912, bytes: 1179648]\n",
            "network/resblock2_0/conv_0/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/conv_init/conv2d/kernel:0 (float32_ref 1x1x128x256) [32768, bytes: 131072]\n",
            "network/resblock2_0/conv_init/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/batch_norm_1/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/batch_norm_1/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
            "network/resblock2_0/conv_1/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_0/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_0/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
            "network/resblock2_1/conv_0/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_1/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/batch_norm_1/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock2_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x256x256) [589824, bytes: 2359296]\n",
            "network/resblock2_1/conv_1/conv2d/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock_3_0/batch_norm_0/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock_3_0/batch_norm_0/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "network/resblock_3_0/conv_0/conv2d/kernel:0 (float32_ref 3x3x256x512) [1179648, bytes: 4718592]\n",
            "network/resblock_3_0/conv_0/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/conv_init/conv2d/kernel:0 (float32_ref 1x1x256x512) [131072, bytes: 524288]\n",
            "network/resblock_3_0/conv_init/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/batch_norm_1/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/batch_norm_1/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_0/conv_1/conv2d/kernel:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
            "network/resblock_3_0/conv_1/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_0/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_0/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/conv_0/conv2d/kernel:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
            "network/resblock_3_1/conv_0/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_1/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/batch_norm_1/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/resblock_3_1/conv_1/conv2d/kernel:0 (float32_ref 3x3x512x512) [2359296, bytes: 9437184]\n",
            "network/resblock_3_1/conv_1/conv2d/bias:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/batch_norm/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/batch_norm/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
            "network/logit/dense/kernel:0 (float32_ref 512x10) [5120, bytes: 20480]\n",
            "network/logit/dense/bias:0 (float32_ref 10) [10, bytes: 40]\n",
            "Total size of variables: 11176970\n",
            "Total bytes of variables: 44707880\n",
            " [*] Reading checkpoints...\n",
            "INFO:tensorflow:Restoring parameters from /ResNet18_cifar10_128_0.01/ResNet.model-15600\n",
            " [*] Success to read ResNet.model-15600\n",
            " [*] Load SUCCESS\n",
            "Epoch: [40]  time: 0.0000, learning_rate : 0.0001\n",
            "Epoch: [40] [  389/  390] time: 539.3040, train_accuracy: 0.91, test_accuracy: 0.88, learning_rate : 0.0001\n",
            "Epoch: [41]  time: 539.6340, learning_rate : 0.0001\n",
            "Epoch: [41] [  389/  390] time: 1073.5033, train_accuracy: 0.91, test_accuracy: 0.88, learning_rate : 0.0001\n",
            "Epoch: [42]  time: 1073.7174, learning_rate : 0.0001\n",
            "Epoch: [42] [  389/  390] time: 1607.4859, train_accuracy: 0.93, test_accuracy: 0.88, learning_rate : 0.0001\n",
            "Epoch: [43]  time: 1607.7006, learning_rate : 0.0001\n",
            "Epoch: [43] [  389/  390] time: 2141.3575, train_accuracy: 0.93, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [44]  time: 2141.5696, learning_rate : 0.0001\n",
            "Epoch: [44] [  389/  390] time: 2675.4006, train_accuracy: 0.90, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [45]  time: 2675.6155, learning_rate : 0.0001\n",
            "Epoch: [45] [  389/  390] time: 3209.3646, train_accuracy: 0.93, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Epoch: [46]  time: 3209.5919, learning_rate : 0.0001\n",
            "Epoch: [46] [  389/  390] time: 3743.3749, train_accuracy: 0.93, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [47]  time: 3743.6068, learning_rate : 0.0001\n",
            "Epoch: [47] [  389/  390] time: 4277.4506, train_accuracy: 0.91, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [48]  time: 4277.6810, learning_rate : 0.0001\n",
            "Epoch: [48] [  389/  390] time: 4811.5137, train_accuracy: 0.91, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [49]  time: 4811.7421, learning_rate : 0.0001\n",
            "Epoch: [49] [  389/  390] time: 5345.5647, train_accuracy: 0.91, test_accuracy: 0.88, learning_rate : 0.0001\n",
            "Epoch: [50]  time: 5345.7919, learning_rate : 0.0001\n",
            "Epoch: [50] [  389/  390] time: 5879.6909, train_accuracy: 0.92, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [51]  time: 5879.9202, learning_rate : 0.0001\n",
            "Epoch: [51] [  389/  390] time: 6413.6599, train_accuracy: 0.91, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [52]  time: 6413.8936, learning_rate : 0.0001\n",
            "Epoch: [52] [  389/  390] time: 6947.5854, train_accuracy: 0.95, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [53]  time: 6947.8155, learning_rate : 0.0001\n",
            "Epoch: [53] [  389/  390] time: 7481.6217, train_accuracy: 0.91, test_accuracy: 0.89, learning_rate : 0.0001\n",
            "Epoch: [54]  time: 7481.8513, learning_rate : 0.0001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb4Fr3QUSOsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "17312748-66dd-45e0-944b-14c36cd59df9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# from google.colab import files\n",
        "# files.download('ResNet18_cifar10_128_0.01.zip')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00tsh7tQVrfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e348516d-316c-4584-9224-932c60b79be5"
      },
      "source": [
        "cd '/content/drive'"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPdg_vt7V1d-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "ce4f863c-c67e-4fbc-b83a-917bb272a73f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin\n",
            "boot\n",
            "content\n",
            "datalab\n",
            "dev\n",
            "dlib-19.18.0-cp27-cp27mu-linux_x86_64.whl\n",
            "dlib-19.18.0-cp36-cp36m-linux_x86_64.whl\n",
            "etc\n",
            "home\n",
            "lib\n",
            "lib32\n",
            "lib64\n",
            "media\n",
            "mnt\n",
            "opt\n",
            "proc\n",
            "ResNet18_cifar10_128_0.01\n",
            "ResNet18_cifar10_128_0.01_10epochs.zip\n",
            "root\n",
            "run\n",
            "sbin\n",
            "srv\n",
            "swift\n",
            "sys\n",
            "tensorflow-2.1.0\n",
            "tmp\n",
            "tools\n",
            "usr\n",
            "var\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29k-vPm-V6xB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "472ba677-dcf3-42f8-fb41-937bcf935101"
      },
      "source": [
        "!zip -r ResNet18_cifar10_128_0.01.zip ResNet18_cifar10_128_0.01"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: ResNet18_cifar10_128_0.01/ (stored 0%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3511.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-14430.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2341.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3121.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-14820.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2731.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2731.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/events.out.tfevents.1576989402.12f1060b182d (deflated 87%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-14820.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3121.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3901.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2341.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3511.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2731.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3121.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-15600.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3901.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-15210.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-14430.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-15210.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3511.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-14040.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-15210.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-15600.meta (deflated 92%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-14820.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/checkpoint (deflated 78%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-3901.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/events.out.tfevents.1576997124.12f1060b182d (deflated 82%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-14040.meta (deflated 93%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-14040.index (deflated 67%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-2341.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-15600.data-00000-of-00001 (deflated 7%)\n",
            "  adding: ResNet18_cifar10_128_0.01/ResNet.model-14430.data-00000-of-00001 (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EFlQxJ3Ug6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ResNet18_cifar10_128_0.01.zip '/content/drive/My Drive/ResNet18_cifar10_128_0.01.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s45Py_YfXMDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ResNet18_cifar10_128_0.01_10epochs.zip '/content/drive/My Drive/ResNet18_cifar10_128_0.01_10epochs.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RScynImJXa72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "a2f715f4-4738-4730-b0bf-303b9b23fbb0"
      },
      "source": [
        " with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        args=()\n",
        "        cnn = ResNet(sess, args)\n",
        "\n",
        "        # build graph\n",
        "        cnn.build_model()\n",
        "\n",
        "        # show network architecture\n",
        "        show_all_variables()\n",
        "\n",
        "        #if args.phase == 'train' :\n",
        "        # launch the graph in a session\n",
        "        cnn.train()\n",
        "\n",
        "        print(\" [*] Training finished! \\n\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120.70756512369792\n",
            "64.1500758911213\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-0c1eeb954103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0;31m# build graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m        \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m        \u001b[0;31m# show network architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3c56fb755ce5>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;34m\"\"\" Model \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_inptus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_inptus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3c56fb755ce5>\u001b[0m in \u001b[0;36mnetwork\u001b[0;34m(self, x, is_training, reuse)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c155637b2286>\u001b[0m in \u001b[0;36mconv\u001b[0;34m(x, channels, kernel, stride, padding, use_bias, scope)\u001b[0m\n\u001b[1;32m     21\u001b[0m                              \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                              \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                              strides=stride, use_bias=use_bias, padding=padding)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    422\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m       _scope=name)\n\u001b[0;32m--> 424\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1698\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m     \"\"\"\n\u001b[0;32m-> 1700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    530\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable network/conv/conv2d/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
          ]
        }
      ]
    }
  ]
}